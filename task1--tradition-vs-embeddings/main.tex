% Status as of 04.08.2020 15:00 CET bm
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[review]{elsarticle}

%% Was trying to change the title of 'abstract' to something else.. but failed miserably
%% probably the elsarticle package's fault...
% \usepackage{etoolbox}
% \patchcmd{<cmd>}{<search>}{<replace>}{<success>}{<failure>}
% \patchcmd{\abstract}{Abstract}{Summary}{}{}
% \renewcommand{\abstractname}{Summary}

\usepackage{lineno}
\usepackage{easylist}
\usepackage{amssymb}
\usepackage{float}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{textcomp}
\usepackage{verbatim}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{mathtools}
%\usepackage[ampersand]{easylist}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\usepackage{tikzscale}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{tikz-qtree,tikz-qtree-compat}
\usetikzlibrary{calc}
\modulolinenumbers[5]
\journal{Journal of cool mini-projects}


%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}

%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{frontmatter}

\title{Mini-project A: \\ Traditional recommenders vs. (graph) neural embeddings }

\author[TUG,MUG]{Supervisor: Bernd Malle}
\author[TUG,MUG]{\small \\Professor: Andreas Holzinger}
% \cortext[mycorrespondingauthor]{Corresponding author}
% \ead{bernd.malle@medunigraz.at}

\address[TUG]{Graz University of Technology, Austria}
\address[MUG]{Medical University Graz, Austria}


\begin{abstract}

In order to improve or complement traditional recommendation approaches, you will compare the results of pairwise similarity measures obtained via Matrix Factorization and Random Walks with similarities measured in a neural embedding space computed via word2vec/fasttext or graph representation learning (GRL).

\end{abstract}

\begin{keyword}
similarity, graph representation learning, graph embeddings, recommendations
\end{keyword}

\end{frontmatter}


\section{Motivation}
\label{sect:motivation}

Recommender systems are popular and fundamental in many modern application areas, ranging from "simple" product recommendations on Amazon \& Netflix to highly-complex and mission-critical decision support systems in healthcare, financial transactions and control systems. Often recommenders are based on (dis)similarity of items, which can be derived from transactional information (\textit{user x bought product y at time z}). Since a global matrix representing such interaction is highly sparse, techniques like Matrix Factorization are often used to break down these (implicit) matrix into smaller, dense building blocks more amenable to efficient processing. A competing approach is to model the item domain \& transactions as embeddings, i.e. low-dimensional, dense representations of the input domain by feeding a neural network with examples of positive / negative correlations, significantly accelerating the process. These embeddings have proven extremely successful in NLP, but have been applied to a variety of other domains as well, including chemistry and genetics.

\section{Goals}
\label{sect:goals}

This mini-project shall give you the opportunity to apply 3-4 different techniques of constructing a similarity space from a simple (Amazon) product dataset. The emphasis lies on learning how to

\begin{itemize}
  \item construct a corpus amenable to processing by word2vec, fasttext etc.
  \item construct a simple graph structure out of the raw corpus, so that we can apply pre-determined graph algorithms on it (similarity measures via random walks, graph kernels, jaccard distance etc.).
  \item compute graph-based embeddings (node representations) via an algorithm like GraphSAGE
  \item compare the similarity spaces obtained by above methods to develop a feeling for their strenghts and weaknesses.
\end{itemize}

% \begin{figure}[hbt!]
% \begin{center}
% % \includegraphics[width=\textwidth]{figures/transductive_learning}
% \end{center}
% \caption{General overview of a (distributed) graph-based ML pipeline in the medical domain}
% \label{fig:Figure-1}
% \end{figure}


\section{Procedure}
\label{sect:procedure}

Starting with the raw corpus, your task will encompass an in

\bibliography{references}

\end{document}
